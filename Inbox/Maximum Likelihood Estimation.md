

2024-10-13 16:01

Status: 

Tags: #video 

# Maximum Likelihood Estimation

we want to estimate $P(\omega_{i}|x)$. e.g. given a sample data $x$ what is the probability that it belongs to class $\omega_{i}$
we can  use [[bayes rule]].
first we need to learn the likelihood $\forall i, P(x|\omega_{i })$ 
## Salmon catfish example
the fishes have 2 features. 
- length
- lightness
a vector of size 2. for example
$\begin{bmatrix}2 \\ 5\end{bmatrix}$, $\begin{bmatrix}4.5 \\ 2.3\end{bmatrix}$ $\dots$
we can observe each of the examples  and build a histogram for every class.
the number of bins will be: $S^d$. and we'll have the PDF for every class(likelihood)
$S$ is the number of steps in each dimension and $d$ is the number of dimensions.
its grow exponentially. **not practical**.
### Model base approach
we can assume that the likelihood probability was created from a known parametric distribution.
lets assume that we think that the data was generated from a gaussian Distribution: $P(x|w_{j})\sim N( \mu_{j}, \sum_{j}^{} )$
 the goal now is to estimate the parameters  $\mu_{j}=?$, $\sum_{j}^{}=?$ so its best fits the observations we have.
 with this approach we need just a small sample of observations and not the *whole* data distribution.
 that approach is *Maximum Likelihood Estimation*
 the data $D$ with $N$ samples
 $D=x_{1},x_{2},\dots,x_{N}$ , $x_{i}\in \mathbb{R}^{d}$ generated by $P(x|\theta)$
 $\theta$ is our model parameters. in our example $\mu_{j}, \sum_{j}$

for every $x_{k}$ we estimate  $\theta_{k}$ , hence we assume $x_{k}\sim N(\theta_{k})$ .
for simplicity we'll use just one parameter: $\mu$. assuming that $\sum_{j}$ is known.
 
![[Maximum Likelihood Estimation1-1.png]]

we need to find the $\theta$ (in our case $\mu$), that fits the data the best.
to do that we'll look at the *joint density function* $p(D_{w_{k}}|\theta)$ which also called the likelihood of $\theta$ regarding $D$. which is the product of all density functions.
$p(D_{w_{k}}|\theta)=\displaystyle \prod_{k=1}^{N} \frac{1}{\sqrt{ 2\pi}}\exp\left[ -\frac{1}{2}(x_{k}-\theta)^2 \right]$

$\underset{\theta} {\mathrm{argmax}} ~p(D_{w_{k}}|\theta)=\hat{\theta_{k}}=\frac{1}{N}\displaystyle \sum_{k=1}^{N}x_{k}$. 
Which is the mean of the samples(makes sense...)
$\hat{\theta}_{k}$ is called the *Maximum Likelihood Estimator*
having $\hat{\theta}_{k}$ we can calculate $p(x|w_{k})$.
to build the *classifier* based on [[bayes rule|bayes theory]] we go though this process of finding $\theta$ for each class: $p(x|\omega_{salmon}),p(x|\omega_{catfish})$
in addition we assume we know the prior  $p(\omega_{salmon}),p(\omega_{catfish})$
we find then : $\underset{j} {\mathrm{argmax}} ~\log[p(x|\omega_{j}]+\log[p(\omega_{j})]$
predicting class $j$.


## References

https://www.youtube.com/watch?v=sguol03tfWo&t=319s
