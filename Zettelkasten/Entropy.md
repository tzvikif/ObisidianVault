

2024-07-18 09:10

Status:

Tags: 


## Entropy

$$
H(X)
$$
the Expectation of the "surprise" of the distribution of **Random Variable** $X$

$1/logP(x)$ is the "surprise" of event $x$ occurred. 

![[Pasted image 20240721170028.png|400]]


$P(x)$ is the probability to get x
we go over all the value of y and get the expectation of the surprise:

$H(X)=-\displaystyle \sum\limits_{x \in X}p(x)\log p(x)$

conditional entropy:
$H(X|Y)=-\displaystyle \sum\limits_{y\in Y}p(y)\displaystyle \sum\limits_{x \in X}p(x|y)\log p(x|y)$
## References

