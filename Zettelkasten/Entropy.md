

2024-07-18 09:10

Status:

Tags: 


# Entropy

the *Entropy* of an *ensemble* is the average [[Shannon information content of an outcome|Shannon information content]]
$H(X)=\displaystyle \sum_{x\in X}p(x)\cdot log_2\frac{1}{p(x)}$ bits

*ensemble* is the [[PDF]] of the *random variable* $X$ (collection of probabilities for each outcome of $X$)

## conditional entropy:

$H(X|Y)=-\displaystyle \sum\limits_{y\in Y}p(y)\displaystyle \sum\limits_{x \in X}p(x|y)\log p(x|y)$
## References

