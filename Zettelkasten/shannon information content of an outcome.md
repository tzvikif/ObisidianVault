

2024-09-29 12:17

Status: 

Tags:

## shannon information content of an outcome

$a_i$ is a letter from the alphabet.
$h(x=a_i)=log_2\frac{1}{p(x=a_{i)}}bits$

- $h$ is *additive* for **independent** random variables.
	e.g  random variables: $XY$
	$p(x,y)=p(x)p(y)$ for all $x,y$
	then $h(h,y)=log_2\frac{1}{p(x,y)}=log_2\frac{1}{p(x)}+log_2\frac{1}{p(y)}$
### definition
the [[Entropy]] of an *ensemble* is the average Shannon information content
$H(x)=\displaystyle \sum_{x}p(x)\cdot log_2\frac{1}{p(x)}$ bits





## References

